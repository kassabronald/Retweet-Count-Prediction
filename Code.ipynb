{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHoF9GPmyWehsllsoAcGkI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2ElEgo0bgYpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670428086452,"user_tz":-60,"elapsed":3021,"user":{"displayName":"Ronald Kassab","userId":"12039094409208064223"}},"outputId":"c298c304-a93a-4d06-81da-db5105d671a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import torch\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import csv\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import LSTM, Activation, Dropout, Dense, Input, Conv1D, MaxPooling1D, Flatten, BatchNormalization, LeakyReLU,concatenate\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.svm import SVC, LinearSVC, NuSVC\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.metrics import mean_absolute_error\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n","\n","\n","\n","#only if you are using google clollab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#end of block\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #if you have a GPU with CUDA installed, this may speed up computation\n","\n","\n","# Load the training data\n","#data = pd.read(\"train.csv\")\n","\n","#For running the deep learning model might need to take less than the whole dataset otherwise training takes too much time\n","data = pd.read_csv(\"/content/drive/MyDrive/DATA_ML/train.csv\")\n","\n","\n","  "]},{"cell_type":"code","source":["eval_data = pd.read_csv(\"/content/drive/MyDrive/DATA_ML/evaluation.csv\")\n","X_off_pipeline = eval_data.loc[:, ~eval_data.columns.isin(['urls','TweetID'])] #Run with pipeline\n","X_off_Neural_Net = eval_data.loc[:, ~eval_data.columns.isin(['urls','hashtags','mentions','TweetID'])]\n","X_off_XGB = eval_data.drop(['text', 'urls', 'mentions', 'hashtags'], axis=1)\n","\n","#----------------------------------------------------------------------Function to submit prediction-------------------------------------------------\n","\n","def submit(nameFile, y_pred):\n","  with open(nameFile, 'w') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"TweetID\", \"retweets_count\"])\n","    for index, prediction in enumerate(y_pred):\n","        writer.writerow([str(eval_data['TweetID'].iloc[index]) , str(int(prediction))])\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"CHQqno8aiHUt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------------Pipeline Model---------------------------------------------------------\n","\n","\n","\n","\n","#Transormers \n","numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n","categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),('onehot', OneHotEncoder(handle_unknown='ignore'))])\n","\n","X = data\n","y = data.loc[:,['retweets_count']]\n","#Splitting the data into test-train\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n","\n","\n","#selecting the features we care about\n","numeric_features = data.select_dtypes(include=['int64', 'float64']).drop(['TweetID', 'retweets_count'], axis = 1).columns\n","categorical_features = data.select_dtypes(include=['object']).drop(['urls'], axis=1).columns\n","\n","\n","\n","#Using a column transformer to preprocess the data as specified by the above feature selector\n","preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),('cat', categorical_transformer, categorical_features)])\n","\n","\n","params = {\n","    \"n_estimators\": 850,\n","    \"learning_rate\": 0.05,\n","    \"loss\": \"absolute_error\",\n","}\n","\n","\n","#Can be changed to anything such as RandomForestRegressor(), KNeighborsRegressor(), DecisionTreeRegressor() \n","classifiers = [GradientBoostingRegressor(**params)]\n","\n","\n","#Creating our pipeline model\n","rf = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('classifier', classifiers[0])])\n","\n","#fit on X_train and y_train\n","rf.fit(X_train, y_train.values.ravel())\n","#Getting the prediction\n","y_pred = rf.predict(X_test)\n","  \n","print(\"Prediction error:\", mean_absolute_error(y_true=y_test, y_pred=y_pred))\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"0qCAb_A0g-FE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------------------Evaluate with pipeline---------------------------------------------------\n","y_pred_pipeline = rf.predict(X_off_pipeline)\n","submit(\"/content/drive/MyDrive/DATA_ML/Prediction-Pipeline-GradientBooster.txt\", y_pred_pipeline)"],"metadata":{"id":"WWWW8GaAInnd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#----------------------------------------------------------------------------Function to easily get data-------------------------------------------\n","\n","\n","def getXY_tot(df):\n","  X = df.loc[:, ~df.columns.isin(['urls','hashtags','mentions','retweets_count','TweetID'])]\n","  y = df.loc[:,['retweets_count']]\n","  return X, y\n","\n","def normalize(P):\n","  means = [P[col].mean() for col in P]\n","  stds = [P[col].std() for col in P]\n","\n","  return (P-means)/stds"],"metadata":{"id":"c2309ujpioXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------------------------Splitting the data into textual and numerical data---------------------------------\n","\n","X, y = getXY_tot(data)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n","X_train_t = X_train.loc[:,['text']]\n","X_train_n = X_train.loc[:, ~X_train.columns.isin(['text'])]\n","X_test_t = X_test.loc[:,['text']]\n","X_test_n = X_test.loc[:, ~X_test.columns.isin(['text'])]"],"metadata":{"id":"q33uTVFqieY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------------------------Creating our Embedding Layer for the CNN------------------------------------------------\n","\n","tmp = X['text']\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(tmp)\n","def embedding_for_vocab(filepath, word_index,embedding_dim):\n","    vocab_size = len(word_index) + 1\n","    # Adding again 1 because of reserved 0 index\n","    embedding_matrix_vocab = np.zeros((vocab_size,embedding_dim))\n","    with open(filepath, encoding=\"utf8\") as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word]\n","                embedding_matrix_vocab[idx] = np.array(\n","                    vector, dtype=np.float32)[:embedding_dim]\n","    return embedding_matrix_vocab\n","\n","\n","def getEmbeddingLayer(embedding_dim, maxLen,tokenizer):\n","  embedding_matrix_vocab = embedding_for_vocab('/content/drive/MyDrive/DATA_ML/glove.6B/glove.6B.50d.txt', tokenizer.word_index,embedding_dim)\n","  embed_vector_len  = embedding_matrix_vocab[0].shape[0]\n","  words_to_index = tokenizer.word_index\n","  vocab_len = len(words_to_index)+1\n","  emb_matrix = np.zeros((vocab_len, embed_vector_len))\n","  for word, index in words_to_index.items():\n","    embedding_vector = embedding_matrix_vocab[index]\n","    if embedding_vector is not None:\n","      emb_matrix[index, :] = embedding_vector\n","  embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)\n","  return embedding_layer\n","\n","maxLen = 80\n","embedding_dim = 50\n","embedding_layer = getEmbeddingLayer(embedding_dim,maxLen,tokenizer)\n"],"metadata":{"id":"fDjRRseci992"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#---------------------------------------------------------------------------Concatenated Model-----------------------------------------------------------\n","\n","\n","\n","def concatModel(in_shape_t, in_shape_n, hidde_dim, out_shape):\n","  x_in_t = Input(shape=in_shape_t)\n","  x = embedding_layer(x_in_t)\n","  x = ((Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')))(x)\n","  x = (BatchNormalization())(x)\n","  x = ((Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')))(x)\n","  x = (BatchNormalization())(x)\n","  x = ((Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')))(x)\n","  x = (BatchNormalization())(x)\n","  x = ((Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')))(x)\n","  x = (BatchNormalization())(x)\n","  x = (MaxPooling1D(pool_size=2))(x)\n","  x = Flatten()(x)\n","  x = Dense(64, activation = 'relu')(x)\n","  x = (BatchNormalization())(x)\n","  x = Dense(64, activation = 'relu')(x)\n","  x = (BatchNormalization())(x)\n","  x = Dense(64, activation = 'relu')(x)\n","  x = (BatchNormalization())(x)\n","  x = Dropout(0.5)(x)\n","  x_out = Dense(out_shape, activation = 'relu')(x)\n","  \n","  x_in_n = Input(shape=in_shape_n)\n","  xx = Dense(hidde_dim)(x_in_n)\n","  xx = LeakyReLU(alpha=0.5)(xx)\n","  xx = Dense(hidde_dim)(xx)\n","  xx = LeakyReLU(alpha=0.5)(xx)\n","  xx = Dense(hidde_dim)(xx)\n","  xx_out = LeakyReLU(alpha=0.5)(xx)\n","\n","\n","  merged = concatenate([x_out,xx_out],name=\"concatenated_layer\")\n","  output_layer = Dense(out_shape, activation = \"relu\", name = \"output_layer\")(merged)\n","  model = Model(inputs = [x_in_t, x_in_n], outputs = output_layer)\n","\n","  return model\n","\n","\n","\n"],"metadata":{"id":"Mab8wwYAkAk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#----------------------------------------------------------------------Training the Model----------------------------------------------\n","\n","\n","tmp = X_train_t['text']\n","X_train_indices = tokenizer.texts_to_sequences(tmp)\n","X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n","X_train_n_norm = normalize(X_train_n)\n","X_test_n_norm = normalize(X_test_n)\n","adam = tf.keras.optimizers.Adam(learning_rate = 0.01)\n","model_concat = concatModel((maxLen,), (X_train_n_norm.shape[1],), 256, y_train.shape[1])\n","model_concat.compile(optimizer=adam, loss='mean_absolute_error', metrics=['accuracy'])\n","model_concat.fit([X_train_indices, X_train_n_norm], y_train, batch_size = 1024, epochs = 100)"],"metadata":{"id":"HDKFpKLFkRI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------------------------Testing model-------------------------------------------------------------\n","\n","tmp = X_test_t['text']\n","X_test_indices = tokenizer.texts_to_sequences(tmp)\n","X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n","pred_n = model_concat.predict([X_test_indices,X_test_n_norm])\n","pred_n = [int(value) if value >= 0 else 0 for value in pred_n]\n","print(\"Prediction error:\", mean_absolute_error(y_true=y_test, y_pred=pred_n))"],"metadata":{"id":"2Io1eQzNk3o7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------------------------------------------------------Evaluate with Neural Network----------------------------------------------\n","\n","X_off_Neural_Net_t = X_off_Neural_Net.loc[:,['text']]\n","X_off_Neural_Net_n = X_off_Neural_Net.loc[:, ~X_off_Neural_Net.columns.isin(['text'])]\n","tmp_t = X_off_Neural_Net_t['text']\n","X_off_indices = tokenizer.texts_to_sequences(tmp_t)\n","X_off_indices = pad_sequences(X_off_indices, maxlen=maxLen, padding='post')\n","y_pred_Neural_Net = model_concat.predict([X_off_indices, X_off_Neural_Net_n])\n","submit(\"/content/drive/MyDrive/DATA_ML/Prediction-Neural-Net.txt\", y_pred_Neural_Net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUXZH3EOKDuo","executionInfo":{"status":"ok","timestamp":1670426696360,"user_tz":-60,"elapsed":85997,"user":{"displayName":"Ronald Kassab","userId":"12039094409208064223"}},"outputId":"acfabecd-6f08-4c57-e159-9e79f70163eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3688/3688 [==============================] - 44s 12ms/step\n"]}]},{"cell_type":"code","source":["#-------------------------------------------------------------------------XGBooster----------------------------------------------------------\n","import pickle\n","import xgboost as xgb\n","from os.path import exists\n","from xgboost import XGBRegressor\n","\n","\n","def train():\n","    X = data.drop(['text', 'retweets_count', 'urls', 'mentions', 'hashtags'], axis=1)\n","    Y = data['retweets_count']\n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=True)\n","    evalset = [(X_train, Y_train), (X_test,Y_test)]\n","    regressor = XGBRegressor(objective = 'reg:linear' , learning_rate=0.1, max_depth = 3, n_estimators = 360)\n","    regressor.fit(X_train, Y_train,eval_metric = 'mae', eval_set=evalset)\n","    predictions = regressor.predict(X_test)\n","    results = regressor.evals_result()\n","    plt.plot(results['validation_0']['mae'], label='train')\n","    plt.plot(results['validation_1']['mae'], label='test')\n","    plt.legend()\n","    plt.show()\n","    error = mean_absolute_error(y_true=Y_test, y_pred=predictions)\n","    print('Prediction error:', error)\n","    return regressor\n","\n","\n","def test(regressor, X):\n","    predictions = regressor.predict(X)\n","    return predictions\n","\n","\n"],"metadata":{"id":"l7pYPX5RlGRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["regressor = train()\n","y_pred_XGB = test(regressor, X_off_XGB)\n","submit(\"/content/drive/MyDrive/DATA_ML/Prediction-XGB.txt\", y_pred_XGB)"],"metadata":{"id":"cQeYnMMTMaFD"},"execution_count":null,"outputs":[]}]}